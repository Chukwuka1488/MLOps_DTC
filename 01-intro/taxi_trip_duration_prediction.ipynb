{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chukwuka1488/MLOps_DTC/blob/main/01-intro/taxi_trip_duration_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GH0pgJflx3l",
        "outputId": "f2311833-67d3-42e7-f83a-95433d1b90c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.12\n"
          ]
        }
      ],
      "source": [
        "!python -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Dd9qxzNRyZIR"
      },
      "outputs": [],
      "source": [
        "# !pip install cudf-cu11 --extra-index-url=https://pypi.ngc.nvidia.com\n",
        "# !rm -rf /usr/local/lib/python3.8/dist-packages/cupy*\n",
        "# !pip install cupy-cuda11x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uZwTmIba9iiB"
      },
      "outputs": [],
      "source": [
        "# import cudf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiMQTHEI3rR8",
        "outputId": "5f2e0ce0-3078-43f7-a9ed-c100ac32d41f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyarrow\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "kHtxqzQ2prxP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# for parquet files\n",
        "import pyarrow as pa\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "z8bBmjkcGzhq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a model"
      ],
      "metadata": {
        "id": "KkbWpRabImpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(filepath: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads a Parquet file into a Pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        filepath: The path to the Parquet file.\n",
        "\n",
        "    Returns:\n",
        "        A Pandas DataFrame.\n",
        "    \"\"\"\n",
        "    df = pd.read_parquet(filepath)\n",
        "    return df\n",
        "\n"
      ],
      "metadata": {
        "id": "Lo4QrkAE_M04"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_duration(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the 'duration' column in minutes from 'lpep_dropoff_datetime'\n",
        "    and 'lpep_pickup_datetime' columns.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame with 'lpep_dropoff_datetime' and 'lpep_pickup_datetime'.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame with the 'duration' column added.\n",
        "    \"\"\"\n",
        "    df['duration'] = df['lpep_dropoff_datetime'] - df['lpep_pickup_datetime']\n",
        "    df['duration'] = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
        "    return df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "C_iShtKH_WOs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_duration_outliers(df: pd.DataFrame, min_duration: float, max_duration: float) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters the DataFrame to keep records where 'duration' is within a specified range.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame with a 'duration' column.\n",
        "        min_duration: The minimum acceptable duration (inclusive).\n",
        "        max_duration: The maximum acceptable duration (inclusive).\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame with 'duration' outliers removed.\n",
        "    \"\"\"\n",
        "    df_filtered = df[(df['duration'] >= min_duration) & (df['duration'] <= max_duration)].copy()\n",
        "    return df_filtered\n",
        "\n"
      ],
      "metadata": {
        "id": "nxatRVCmDH1C"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_fraction_remaining(original_df: pd.DataFrame, filtered_df: pd.DataFrame) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the fraction of records remaining after filtering.\n",
        "\n",
        "    Args:\n",
        "        original_df: The DataFrame before filtering.\n",
        "        filtered_df: The DataFrame after filtering.\n",
        "\n",
        "    Returns:\n",
        "        The fraction of records remaining.\n",
        "    \"\"\"\n",
        "    total_records_before = len(original_df)\n",
        "    total_records_after = len(filtered_df)\n",
        "    fraction_remaining = total_records_after / total_records_before\n",
        "    return fraction_remaining\n",
        "\n"
      ],
      "metadata": {
        "id": "FsYrf4V9GCpY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_features_train(df: pd.DataFrame, categorical_cols: list, numerical_cols: list):\n",
        "    \"\"\"\n",
        "    Prepares features for model training by converting categorical columns to string\n",
        "    and applying DictVectorizer. It fits the DictVectorizer.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame (training data).\n",
        "        categorical_cols: A list of column names to treat as categorical.\n",
        "        numerical_cols: A list of column names to treat as numerical.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "            - X_transformed: The feature matrix (sparse matrix).\n",
        "            - dv: The fitted DictVectorizer object.\n",
        "    \"\"\"\n",
        "    df[categorical_cols] = df[categorical_cols].astype(str)\n",
        "    train_dicts = df[categorical_cols + numerical_cols].to_dict(orient='records')\n",
        "    dv = DictVectorizer()\n",
        "    X_train = dv.fit_transform(train_dicts)\n",
        "    return X_train, dv\n",
        "\n",
        "def prepare_features_val(df: pd.DataFrame, dv: DictVectorizer, categorical_cols: list, numerical_cols: list):\n",
        "    \"\"\"\n",
        "    Prepares features for model validation by converting categorical columns to string\n",
        "    and applying the already fitted DictVectorizer.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame (validation data).\n",
        "        dv: The DictVectorizer fitted on the training data.\n",
        "        categorical_cols: A list of column names to treat as categorical.\n",
        "        numerical_cols: A list of column names to treat as numerical.\n",
        "\n",
        "    Returns:\n",
        "        X_transformed: The feature matrix (sparse matrix).\n",
        "    \"\"\"\n",
        "    df[categorical_cols] = df[categorical_cols].astype(str)\n",
        "    val_dicts = df[categorical_cols + numerical_cols].to_dict(orient='records')\n",
        "    X_val = dv.transform(val_dicts)\n",
        "    return X_val"
      ],
      "metadata": {
        "id": "MZgbL11CQ9zc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X_train, y_train: np.ndarray):\n",
        "    \"\"\"\n",
        "    Trains a Linear Regression model.\n",
        "\n",
        "    Args:\n",
        "        X_train: The training feature matrix.\n",
        "        y_train: The training target variable array.\n",
        "\n",
        "    Returns:\n",
        "        The trained LinearRegression model.\n",
        "    \"\"\"\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model"
      ],
      "metadata": {
        "id": "Ckl7IOaqWckW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X: np.ndarray, y_true: np.ndarray) -> tuple:\n",
        "    \"\"\"\n",
        "    Evaluates a trained model by making predictions and calculating RMSE.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model.\n",
        "        X: The feature matrix for evaluation.\n",
        "        y_true: The true target variable array.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "            - y_pred: The predicted target variable array.\n",
        "            - rmse: The Root Mean Squared Error.\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(X)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    return y_pred, rmse"
      ],
      "metadata": {
        "id": "DFhhOSN5WbAp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_duration_histogram(df: pd.DataFrame, column_name: str, title: str):\n",
        "    \"\"\"\n",
        "    Generates and displays a histogram for a given column in a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame.\n",
        "        column_name: The name of the column to plot.\n",
        "        title: The title of the histogram.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(df[column_name], bins=30, kde=True)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(f'{column_name.replace(\"_\", \" \").title()} (minutes)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WTjzAKJgWZW_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_duration_boxplot(df: pd.DataFrame, column_name: str, title: str):\n",
        "    \"\"\"\n",
        "    Generates and displays a boxplot for a given column in a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame.\n",
        "        column_name: The name of the column to plot.\n",
        "        title: The title of the boxplot.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(x=df[column_name])\n",
        "    plt.title(title)\n",
        "    plt.xlabel(f'{column_name.replace(\"_\", \" \").title()} (minutes)')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "6VdSaVK1WXdc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions_vs_actual(y_pred: np.ndarray, y_actual: np.ndarray):\n",
        "    \"\"\"\n",
        "    Generates and displays a distribution plot comparing predictions and actual values.\n",
        "\n",
        "    Args:\n",
        "        y_pred: The predicted values.\n",
        "        y_actual: The actual values.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(y_pred, label='prediction', kde=True, color='skyblue')\n",
        "    sns.histplot(y_actual, label='actual', kde=True, color='lightcoral')\n",
        "    plt.title('Distribution of Predictions vs. Actual Values')\n",
        "    plt.xlabel('Duration (minutes)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "iqJogGOaWVrt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage (replicating your original script's flow using the functions):\n",
        "if __name__ == '__main__':\n",
        "    # Define data file paths\n",
        "    green_taxi_jan_2025 = \"https://github.com/Chukwuka1488/MLOps_DTC/raw/main/01-intro/nyc_data/green_tripdata_2025-01.parquet\"\n",
        "    green_taxi_feb_2025 = \"https://github.com/Chukwuka1488/MLOps_DTC/raw/main/01-intro/nyc_data/green_tripdata_2025-02.parquet\"\n",
        "\n",
        "    # Load data\n",
        "    df_jan = load_data(green_taxi_jan_2025)\n",
        "    df_feb = load_data(green_taxi_feb_2025)\n",
        "\n",
        "    # --- Preprocessing for Training Data (January) ---\n",
        "    df_jan = compute_duration(df_jan)\n",
        "    df_jan_filtered = filter_duration_outliers(df_jan, 1, 60)\n",
        "\n",
        "    # Calculate fraction remaining for training data\n",
        "    fraction_jan = calculate_fraction_remaining(df_jan, df_jan_filtered)\n",
        "    print(f\"January - Total records before filtering: {len(df_jan)}\")\n",
        "    print(f\"January - Total records after filtering: {len(df_jan_filtered)}\")\n",
        "    print(f\"January - Fraction of records remaining: {fraction_jan:.2f}\")\n",
        "\n",
        "    # --- Preprocessing for Validation Data (February) ---\n",
        "    df_feb = compute_duration(df_feb)\n",
        "    df_feb_filtered = filter_duration_outliers(df_feb, 1, 60)\n",
        "\n",
        "    # Calculate fraction remaining for validation data\n",
        "    fraction_feb = calculate_fraction_remaining(df_feb, df_feb_filtered)\n",
        "    print(f\"\\nFebruary - Total records before filtering: {len(df_feb)}\")\n",
        "    print(f\"February - Total records after filtering: {len(df_feb_filtered)}\")\n",
        "    print(f\"February - Fraction of records remaining: {fraction_feb:.2f}\")\n",
        "\n",
        "    # Define categorical and numerical features (common for both train and val)\n",
        "    categorical = ['PULocationID', 'DOLocationID']\n",
        "    numerical = ['trip_distance']\n",
        "\n",
        "    # Prepare features for training data (fit_transform)\n",
        "    X_train, dv = prepare_features_train(df_jan_filtered, categorical, numerical)\n",
        "    # print(\"\\nTraining Feature names:\\n\", dv.get_feature_names_out())\n",
        "    # print(\"\\nTraining Feature matrix (first 5 rows):\\n\", X_train[:5])\n",
        "    print(\"\\nDimensionality of the training feature matrix (number of columns):\", X_train.shape[1])\n",
        "\n",
        "    # Prepare features for validation data (transform using the fitted dv)\n",
        "    X_val = prepare_features_val(df_feb_filtered, dv, categorical, numerical)\n",
        "    # print(\"\\nValidation Feature matrix (first 5 rows):\\n\", X_val[:5])\n",
        "    print(\"\\nDimensionality of the validation feature matrix (number of columns):\", X_val.shape[1])\n",
        "\n",
        "\n",
        "    # Extract target variable\n",
        "    y_train = df_jan_filtered['duration'].values\n",
        "    y_val = df_feb_filtered['duration'].values\n",
        "\n",
        "    # Train model\n",
        "    model = train_model(X_train, y_train)\n",
        "\n",
        "    # Evaluate model on training data\n",
        "    y_pred_train, rmse_train = evaluate_model(model, X_train, y_train)\n",
        "    print(f\"\\nRMSE of the model on the training data: {rmse_train:.2f} minutes\")\n",
        "\n",
        "    # Evaluate model on validation data\n",
        "    y_pred_val, rmse_val = evaluate_model(model, X_val, y_val)\n",
        "    print(f\"\\nRMSE of the model on the validation data: {rmse_val:.2f} minutes\")\n",
        "\n",
        "    # # Plot distributions for training predictions vs actual\n",
        "    # plot_predictions_vs_actual(y_pred_train, y_train)\n",
        "\n",
        "    # # Plot distributions for validation predictions vs actual\n",
        "    # plot_predictions_vs_actual(y_pred_val, y_val)\n",
        "\n",
        "    # Example of plotting original and filtered duration distributions\n",
        "    # plot_duration_histogram(df_jan, 'duration', 'Distribution of Duration (Original Jan)')\n",
        "    # plot_duration_boxplot(df_jan, 'duration', 'Boxplot of Duration (Original Jan)')\n",
        "    # plot_duration_histogram(df_jan_filtered, 'duration', 'Distribution of Duration (Filtered Jan)')\n",
        "    # plot_duration_boxplot(df_jan_filtered, 'duration', 'Boxplot of Duration (Filtered Jan)')\n",
        "    # plot_duration_histogram(df_feb, 'duration', 'Distribution of Duration (Original Feb)')\n",
        "    # plot_duration_boxplot(df_feb, 'duration', 'Boxplot of Duration (Original Feb)')\n",
        "    # plot_duration_histogram(df_feb_filtered, 'duration', 'Distribution of Duration (Filtered Feb)')\n",
        "    # plot_duration_boxplot(df_feb_filtered, 'duration', 'Boxplot of Duration (Filtered Feb)')\n",
        "\n",
        "    # --- Train and Evaluate Lasso Regression ---\n",
        "    print(\"\\n--- Lasso Regression Results ---\")\n",
        "    # You can experiment with different alpha values.\n",
        "    # A larger alpha means stronger regularization (more coefficients forced to zero).\n",
        "    # A common starting point is 1.0, or you might try smaller values like 0.1, 0.01.\n",
        "    # Instantiate and train the Lasso model directly\n",
        "    lasso_model = Lasso(alpha=0.1)\n",
        "    lasso_model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate Lasso on training data\n",
        "    y_pred_train_lasso, rmse_train_lasso = evaluate_model(lasso_model, X_train, y_train)\n",
        "    print(f\"RMSE of the Lasso model on the training data: {rmse_train_lasso:.2f} minutes\")\n",
        "\n",
        "    # Evaluate Lasso on validation data\n",
        "    y_pred_val_lasso, rmse_val_lasso = evaluate_model(lasso_model, X_val, y_val)\n",
        "    print(f\"RMSE of the Lasso model on the validation data: {rmse_val_lasso:.2f} minutes\")\n",
        "\n",
        "    # You can also inspect the number of non-zero coefficients for Lasso\n",
        "    print(f\"Number of non-zero coefficients for Lasso: {np.sum(lasso_model.coef_ != 0)}\")\n",
        "    print(f\"Total number of features: {X_train.shape[1]}\")\n",
        "\n",
        "    # Plot distributions for Lasso Regression\n",
        "    # plot_predictions_vs_actual(y_pred_train_lasso, y_train, \" (Lasso Regression Train)\")\n",
        "    # plot_predictions_vs_actual(y_pred_val_lasso, y_val, \" (Lasso Regression Validation)\")\n",
        "\n",
        "    # --- Train and Evaluate Ridge Regression ---\n",
        "    print(\"\\n--- Ridge Regression Results ---\")\n",
        "    # You can experiment with different alpha values.\n",
        "    # A larger alpha means stronger regularization (more coefficients are shrunk).\n",
        "    # A common starting point is 1.0, or you might try smaller values like 0.1, 0.01, 10.\n",
        "    # Instantiate and train the Ridge model directly\n",
        "    ridge_model = Ridge(alpha=1) # Instantiate the Ridge model\n",
        "    ridge_model.fit(X_train, y_train) # Fit the model with training data\n",
        "\n",
        "\n",
        "\n",
        "    # Evaluate Ridge on training data\n",
        "    y_pred_train_ridge, rmse_train_ridge = evaluate_model(ridge_model, X_train, y_train)\n",
        "    print(f\"RMSE of the Ridge model on the training data: {rmse_train_ridge:.2f} minutes\")\n",
        "\n",
        "    # Evaluate Ridge on validation data\n",
        "    y_pred_val_ridge, rmse_val_ridge = evaluate_model(ridge_model, X_val, y_val)\n",
        "    print(f\"RMSE of the Ridge model on the validation data: {rmse_val_ridge:.2f} minutes\")\n",
        "\n",
        "    # Ridge typically does not set coefficients to exactly zero, but shrinks them.\n",
        "    # We can still look at the magnitude of coefficients if desired.\n",
        "    # print(f\"Number of non-zero coefficients for Ridge: {np.sum(ridge_model.coef_ != 0)}\") # This will likely be Total number of features\n",
        "\n",
        "    # Plot distributions for Ridge Regression\n",
        "    # plot_predictions_vs_actual(y_pred_train_ridge, y_train, \" (Ridge Regression Train)\")\n",
        "    # plot_predictions_vs_actual(y_pred_val_ridge, y_val, \" (Ridge Regression Validation)\")\n",
        "\n",
        "\n",
        "    # --- Saving the Model and DictVectorizer ---\n",
        "    # Choose the model you want to save. Here, we'll save the Linear Regression model and the DictVectorizer.\n",
        "    # You might choose Lasso or Ridge after tuning, if they perform better.\n",
        "    output_file_model = 'linear_regression_model.bin'\n",
        "    output_file_dv = 'dict_vectorizer.bin'\n",
        "\n",
        "    print(f\"\\nSaving the Linear Regression model to {output_file_model}\")\n",
        "    with open(output_file_model, 'wb') as f_out:\n",
        "        pickle.dump(model, f_out)\n",
        "    print(\"Model saved successfully.\")\n",
        "\n",
        "    print(f\"Saving the DictVectorizer to {output_file_dv}\")\n",
        "    with open(output_file_dv, 'wb') as f_out:\n",
        "        pickle.dump(dv, f_out)\n",
        "    print(\"DictVectorizer saved successfully.\")\n",
        "\n",
        "    # --- Example of Loading the Model and DictVectorizer ---\n",
        "    print(\"\\n--- Testing Model Loading ---\")\n",
        "    try:\n",
        "        with open(output_file_model, 'rb') as f_in:\n",
        "            loaded_model = pickle.load(f_in)\n",
        "        print(\"Model loaded successfully.\")\n",
        "\n",
        "        with open(output_file_dv, 'rb') as f_in:\n",
        "            loaded_dv = pickle.load(f_in)\n",
        "        print(\"DictVectorizer loaded successfully.\")\n",
        "\n",
        "        # You can now use loaded_model and loaded_dv to make predictions on new data\n",
        "        # For example, let's predict on the first 5 samples of the validation data\n",
        "        sample_X_val = X_val[:5]\n",
        "        sample_y_pred = loaded_model.predict(sample_X_val)\n",
        "        print(f\"\\nPredictions on first 5 validation samples using loaded model: {sample_y_pred.round(2)}\")\n",
        "        print(f\"Actual values for first 5 validation samples: {y_val[:5].round(2)}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Saved model or DictVectorizer files not found. Please ensure they were saved correctly.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during loading or prediction: {e}\")\n"
      ],
      "metadata": {
        "id": "XcfEMZFaQ9j0",
        "outputId": "ec3cc66e-ac60-4360-fff7-aab6967418de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "January - Total records before filtering: 48326\n",
            "January - Total records after filtering: 46307\n",
            "January - Fraction of records remaining: 0.96\n",
            "\n",
            "February - Total records before filtering: 46621\n",
            "February - Total records after filtering: 44218\n",
            "February - Fraction of records remaining: 0.95\n",
            "\n",
            "Dimensionality of the training feature matrix (number of columns): 449\n",
            "\n",
            "Dimensionality of the validation feature matrix (number of columns): 449\n",
            "\n",
            "RMSE of the model on the training data: 6.70 minutes\n",
            "\n",
            "RMSE of the model on the validation data: 7.26 minutes\n",
            "\n",
            "--- Lasso Regression Results ---\n",
            "RMSE of the Lasso model on the training data: 8.21 minutes\n",
            "RMSE of the Lasso model on the validation data: 8.69 minutes\n",
            "Number of non-zero coefficients for Lasso: 12\n",
            "Total number of features: 449\n",
            "\n",
            "--- Ridge Regression Results ---\n",
            "RMSE of the Ridge model on the training data: 6.68 minutes\n",
            "RMSE of the Ridge model on the validation data: 7.26 minutes\n",
            "\n",
            "Saving the Linear Regression model to linear_regression_model.bin\n",
            "Model saved successfully.\n",
            "Saving the DictVectorizer to dict_vectorizer.bin\n",
            "DictVectorizer saved successfully.\n",
            "\n",
            "--- Testing Model Loading ---\n",
            "Model loaded successfully.\n",
            "DictVectorizer loaded successfully.\n",
            "\n",
            "Predictions on first 5 validation samples using loaded model: [ 7.02 18.58 18.5  18.64  9.81]\n",
            "Actual values for first 5 validation samples: [ 3.55 27.32 25.47  8.68  9.  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The validation RMSE (7.26 minutes) is slightly higher than the training RMSE (6.70 minutes). This is a common and generally expected outcome. It suggests that the model is performing reasonably well and generalizing to new data without significant overfitting. If the validation RMSE were much higher than the training RMSE, it would be a strong indicator of overfitting, meaning the model learned the training data too well, including its noise, and struggles with new, slightly different data. A difference of about 0.56 minutes is quite acceptable"
      ],
      "metadata": {
        "id": "Q3a8zBeEdVw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the model"
      ],
      "metadata": {
        "id": "cAGzCVC9KL6H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V--Vqp9AfYBs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}